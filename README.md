# DMZ LLM + RAG Sandbox

A self-contained, Docker-based environment simulating a **DMZ Large Language Model** with Retrieval-Augmented Generation (RAG) capabilities.  
Includes a lightweight dev machine, web-based desktop (Webtop), and a Streamlit UI for interaction.

---

## ðŸ“¦ Components

### **Backend**
- **Ollama** â€” Runs the selected LLM model (default: `llama3`).
- **RAG API** â€” FastAPI + ChromaDB backend for document ingestion and semantic search.
- **Vector Store** â€” ChromaDB (embedded inside RAG API container) stores embeddings and metadata.

### **Frontend**
- **Streamlit UI** â€” Web interface to:
  - Send prompts directly to the LLM.
  - Ingest text into the RAG knowledge base.
  - Query the RAG for context-aware answers.

### **Dev Environment**
- **Dev Machine** â€” SSH-accessible Ubuntu container for testing tools in the same network.
- **Webtop** â€” Browser-accessible Ubuntu desktop with a web browser, running fully in Docker.

---

## ðŸ—‚ Project Structure
project-root/
â”œâ”€â”€ docker-compose.yml # Orchestrates all services
â”œâ”€â”€ llm/ # Ollama build & init scripts
â”œâ”€â”€ rag-api/ # RAG backend
â”œâ”€â”€ frontend/ # Streamlit frontend
â”œâ”€â”€ dev-machine/ # CLI dev environment
â””â”€â”€ webtop-config/ # Webtop persistent storage

---

## ðŸ“¦ Services
- **ollama** â€“ LLM service for natural language processing.
- **rag-api** â€“ Backend for document ingestion & retrieval.
- **llm-ui** â€“ Streamlit web app for querying the LLM.
- **dev-machine** â€“ SSH-accessible terminal environment.
- **webtop** â€“ Lightweight remote Linux desktop with browser.

---

## ðŸš€ Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/war1u5/cydex_scenario
cd project-root
```
### 2. Build and start all services:
```bash
docker compose up --build -d
```
### 3. Access the services:
- **Frontend UI**: http://localhost:8501
- **Webtop (GUI):**: http://localhost:3000
- **SSH Dev Machine**: `ssh -p 2222 user@localhost`

