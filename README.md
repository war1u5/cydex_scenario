# DMZ LLM + RAG Sandbox

A self-contained, Docker-based environment simulating a **DMZ Large Language Model** with Retrieval-Augmented Generation (RAG) capabilities.  
Includes a lightweight dev machine, web-based desktop (Webtop), and a Streamlit UI for interaction.

---

## ğŸ“¦ Components

### **Backend**
- **Ollama** â€” Runs the selected LLM model (default: `llama3`).
- **RAG API** â€” FastAPI + ChromaDB backend for document ingestion and semantic search.
- **Vector Store** â€” ChromaDB (embedded inside RAG API container) stores embeddings and metadata.

### **Frontend**
- **Streamlit UI** â€” Web interface to:
  - Send prompts directly to the LLM.
  - Ingest text into the RAG knowledge base.
  - Query the RAG for context-aware answers.

### **Dev Environment**
- **Dev Machine** â€” SSH-accessible Ubuntu container for testing tools in the same network.
- **Webtop** â€” Browser-accessible Ubuntu desktop with a web browser, running fully in Docker.

---

## ğŸ—‚ Project Structure
```bash
project-root/
â”œâ”€â”€ docker-compose.yml # Orchestrates all services
â”œâ”€â”€ llm/ # Ollama build & init scripts
â”œâ”€â”€ rag-api/ # RAG backend
â”œâ”€â”€ frontend/ # Streamlit frontend
â”œâ”€â”€ dev-machine/ # CLI dev environment
â””â”€â”€ webtop-config/ # Webtop persistent storage
```
---

## ğŸ“¦ Services
- **ollama** â€“ LLM service for natural language processing.
- **rag-api** â€“ Backend for document ingestion & retrieval.
- **llm-ui** â€“ Streamlit web app for querying the LLM.
- **dev-machine** â€“ SSH-accessible terminal environment.
- **webtop** â€“ Lightweight remote Linux desktop with browser.

---

## ğŸš€ Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/war1u5/cydex_scenario
cd project-root
```
### 2. Build and start all services:
```bash
docker compose up --build -d
```
### 3. Access the services:
- **Frontend UI**: http://localhost:8501
- **Webtop (GUI):**: http://localhost:3000
- **SSH Dev Machine**: `ssh -p 2222 user@localhost`

## ğŸ“Œ Usage
- Use **/ingest** endpoint in RAG API to store documents.
- Use **/query** endpoint to retrieve context-aware answers.
- Directly interact with the LLM via the Streamlit UI.
- Develop inside `dev-machine` or use `webtop` for GUI tools.

## âš ï¸ Notes
- Ensure Docker and Docker Compose are installed.
- Update `.env` or environment variables for custom configuration.
- The system is meant for local/offline usage; secure before exposing.